{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE with the CIFAR100 dataset\n",
    "Link from fiskemad...\n",
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()])\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 32 # If you are using GPU, you can set the batch size to be 2, 4, 8, 16, 32..., this makes the GPUs work more effciently!\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root='../data/datasetCIFAR100', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(root='../data/datasetCIFAR100', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = trainset.classes # or class_to_idx\n",
    "\n",
    "# Saving of model and checkpoints.\n",
    "saveModelPath = \"../trainedModels/VAE_CIFAR100.pth\"\n",
    "firstTrain = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model and train\n",
    "\n",
    "Models from [here](https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py) and VAE structure from here [git](https://github.com/Jackson-Kang/Pytorch-VAE-tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'VGG9': [64, 'M', 128, 'M', 256, 256, 'M', 512, 'M', 512, 'M'],\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vgg_name, input_dim, latent_dim):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "        cfg[vgg_name] = [input_dim,*cfg[vgg_name]] # first layer always input size of image\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.FC_mean = nn.Conv2d(cfg[vgg_name][-2], latent_dim, kernel_size=1)\n",
    "        self.FC_var = nn.Conv2d(cfg[vgg_name][-2], latent_dim, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        #out = out.view(out.size(0), -1) # Flatten(?)\n",
    "        mean = self.FC_mean(out)\n",
    "        mean = mean.reshape(mean.size(0), mean.size(1), -1)\n",
    "        mean = torch.mean(mean, dim=-1, keepdim=True).unsqueeze(-1) # b, latent, 1, 1\n",
    "\n",
    "        log_var = self.FC_var(out)\n",
    "        log_var = log_var.reshape(log_var.size(0), log_var.size(1), -1)\n",
    "        log_var = torch.mean(log_var, dim=-1, keepdim=True).unsqueeze(-1) # b, latent, 1, 1\n",
    "        return mean, log_var\n",
    "      \n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.GELU()] # changed from ReLU\n",
    "                in_channels = x\n",
    "        #layers += [nn.AvgPool2d(kernel_size=1, stride=1)] giver ikke mening...\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vgg_name, latent_dim, output_dim):\n",
    "\n",
    "        super(Decoder, self).__init__()\n",
    "        cfg[vgg_name] = [latent_dim,*cfg[vgg_name]] # first layer always input size of image\n",
    "        self.latent_dim = latent_dim\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.FC_output = nn.Conv2d(cfg[vgg_name][-2], output_dim, kernel_size=1) # when kernel size is set to 1, this is indeed a FC layer:)\n",
    "        # self.FC_output = nn.Linear(cfg[vgg_name][-2], output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out  = self.features(x)\n",
    "        x_hat = torch.sigmoid(self.FC_output(out))\n",
    "        \n",
    "        return x_hat\n",
    "    \n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = self.latent_dim\n",
    "        for i in range(len(cfg)):\n",
    "            if cfg[i] == 'M':\n",
    "                layers += [nn.ConvTranspose2d(cfg[i-1], cfg[i-1], kernel_size=2, stride=2)] # in decoder, we should upsample the image, instead of downsample it\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, cfg[i], kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(cfg[i]),\n",
    "                           nn.LeakyReLU()] # changed from ReLU\n",
    "                in_channels = cfg[i]\n",
    "        # layers += [nn.AvgPool2d(kernel_size=1, stride=1)] giver ikke mening...\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, Encoder, Decoder):\n",
    "        super(Model, self).__init__()\n",
    "        self.Encoder = Encoder\n",
    "        self.Decoder = Decoder\n",
    "        \n",
    "    def reparameterization(self, mean, var):\n",
    "        epsilon = torch.randn_like(var).to(DEVICE)        # sampling epsilon        \n",
    "        z = mean + var*epsilon                          # reparameterization trick\n",
    "        return z\n",
    "        \n",
    "                \n",
    "    def forward(self, x):\n",
    "        mean, log_var = self.Encoder(x)\n",
    "        z = self.reparameterization(mean, torch.exp(0.5 * log_var)) # takes exponential function (log var -> var)\n",
    "        \n",
    "        # also here, unflatten()\n",
    "        z = z.view(z.size(0), z.size(1), 1, 1)\n",
    "        \n",
    "        x_hat = self.Decoder(z)\n",
    "        \n",
    "        return x_hat, mean, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "channel_size = testset[0][0].shape[0] #Fixed, dim 0 is the feature channel number\n",
    "latent_dim = 256 # hyperparameter\n",
    "lr = 1e-4\n",
    "epochs = 5\n",
    "\n",
    "encoder = Encoder('VGG11',  input_dim=channel_size,     latent_dim=latent_dim)\n",
    "decoder = Decoder('VGG11',  latent_dim=latent_dim,   output_dim = channel_size)\n",
    "\n",
    "model = Model(Encoder=encoder, Decoder=decoder).to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = lr)#optim.Adam(model.parameters(), lr= lr)\n",
    "\n",
    "if firstTrain == True:\n",
    "    startepoch = 0\n",
    "    \n",
    "else:\n",
    "    checkpoint = torch.load(saveModelPath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    startepoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    \n",
    "# Default train mode!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test of dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of inputtorch.Size([2, 3, 32, 32])\n",
      "The mean shape torch.Size([2, 256, 1, 1]), \n",
      "the variance shape torch.Size([2, 256, 1, 1])\n",
      "Latent vector size: torch.Size([2, 256, 1, 1]), and x_hat torch.Size([2, 3, 32, 32])\n",
      "tensor(0.6956, grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = torch.randn(2,3,32,32)\n",
    "print(f\"size of input{x.size()}\")\n",
    "# Encoder test\n",
    "mean, var = encoder(x)\n",
    "print(f\"The mean shape {mean.size()}, \\nthe variance shape {var.size()}\")\n",
    "\n",
    "# Decoder\n",
    "epsilon = torch.randn_like(var).to(DEVICE)        # sampling epsilon        \n",
    "z = mean + var*epsilon  \n",
    "x_hat = decoder(z)\n",
    "\n",
    "print(f\"Latent vector size: {z.size()}, and x_hat {x_hat.size()}\")\n",
    "# Model\n",
    "x_hat, mean, var = model(x)\n",
    "\n",
    "loss_test = nn.functional.binary_cross_entropy(x_hat, x)\n",
    "print(loss_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traning\n",
    "In CIFAR100. First define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_function(x, x_hat, mean, log_var):\n",
    "    reproduction_loss = nn.functional.binary_cross_entropy(x_hat, x)\n",
    "    #KLD      = - 0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())\n",
    "    #KLD = torch.mean( -0.5 * torch.sum(1+ log_var - mean**2 - log_var.exp(),dim=1),dim = 0)\n",
    "    #scale = 0.00025\n",
    "    \n",
    "    #print(f\"Reproduction: {reproduction_loss}, \\tKLD: {KLD.item()}, \\tscaled KLD: {(KLD * scale).item()}, \\tlog_var: {log_var.sum()}\")\n",
    "    print(reproduction_loss.item())\n",
    "    return reproduction_loss #+ KLD*scale #* 0.5 # change? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.694412887096405\n",
      "0.6948010325431824\n",
      "0.6946728229522705\n",
      "0.6949954628944397\n",
      "0.6948586106300354\n",
      "0.6946943402290344\n",
      "0.6947937607765198\n",
      "0.6950170993804932\n",
      "0.6949772834777832\n",
      "0.6949276924133301\n",
      "0.6947660446166992\n",
      "0.6946489810943604\n",
      "0.6946744918823242\n",
      "0.6945489048957825\n",
      "0.6950672268867493\n",
      "0.694856584072113\n",
      "0.6949207186698914\n",
      "0.6952629089355469\n",
      "0.6951518058776855\n",
      "0.6945661902427673\n",
      "0.6939906477928162\n",
      "0.6949846744537354\n",
      "0.6951618194580078\n",
      "0.6946007609367371\n",
      "0.6953015327453613\n",
      "0.6948404312133789\n",
      "0.6950771808624268\n",
      "0.6949770450592041\n",
      "0.6950057148933411\n",
      "0.6943953633308411\n",
      "0.6946597695350647\n",
      "0.6950089931488037\n",
      "0.6946697235107422\n",
      "0.6948482394218445\n",
      "0.6944198608398438\n",
      "0.6949440836906433\n",
      "0.6946679949760437\n",
      "0.6949397921562195\n",
      "0.6943309307098389\n",
      "0.6943848133087158\n",
      "0.6949140429496765\n",
      "0.6943840384483337\n",
      "0.6949601769447327\n",
      "0.6948098540306091\n",
      "0.6947371363639832\n",
      "0.6949717998504639\n",
      "0.6946975588798523\n",
      "0.6946940422058105\n",
      "0.694706380367279\n",
      "0.6947951912879944\n",
      "0.6948379874229431\n",
      "0.6947396397590637\n",
      "0.6950621604919434\n",
      "0.6944320201873779\n",
      "0.694596529006958\n",
      "0.6950168013572693\n",
      "0.6944283843040466\n",
      "0.6948935389518738\n",
      "0.6944279670715332\n",
      "0.6947600245475769\n",
      "0.6945741176605225\n",
      "0.6945992112159729\n",
      "0.6948655247688293\n",
      "0.695054292678833\n",
      "0.6947984099388123\n",
      "0.6943824291229248\n",
      "0.6943459510803223\n",
      "0.6948395371437073\n",
      "0.6948509216308594\n",
      "0.6948065161705017\n",
      "0.6948572993278503\n",
      "0.6945912837982178\n",
      "0.6951178908348083\n",
      "0.694627583026886\n",
      "0.6952990889549255\n",
      "0.6946201920509338\n",
      "0.6946813464164734\n",
      "0.6947559714317322\n",
      "0.6948654651641846\n",
      "0.6946408152580261\n",
      "0.6944367289543152\n",
      "0.6949012875556946\n",
      "0.6948750615119934\n",
      "0.694766104221344\n",
      "0.6946597099304199\n",
      "0.6949095726013184\n",
      "0.6944341659545898\n",
      "0.6947652697563171\n",
      "0.6942134499549866\n",
      "0.6951643824577332\n",
      "0.6954907774925232\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-91950c3642d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstartepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;31m# Save final model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaveModelPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-91950c3642d5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(startepoch, num_epochs, model, loader, plot)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mx_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m#print(f\"min: {min(torch.flatten(x_hat)).item()}, \\tmax: {max(torch.flatten(x_hat)).item()}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-9b0e01439f8d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mx_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-9b0e01439f8d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mout\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mx_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFC_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    393\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 395\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    396\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(startepoch, num_epochs, model, loader, plot : bool = False):\n",
    "    loss_list = []\n",
    "    model.train()\n",
    "\n",
    "    size = len(loader.dataset)\n",
    "    num_batches = len(loader)\n",
    "    \n",
    "    for epoch in range(startepoch,num_epochs):\n",
    "        \n",
    "        for batch_idx, (x, _) in enumerate(loader):\n",
    "        \n",
    "            x = x.to(DEVICE)\n",
    "            \n",
    "            # clear gradients for this training step \n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "            x_hat, mean, log_var = model(x)\n",
    "            \n",
    "            #print(f\"min: {min(torch.flatten(x_hat)).item()}, \\tmax: {max(torch.flatten(x_hat)).item()}\")\n",
    "            loss = loss_function(x, x_hat, mean, log_var)\n",
    "            \n",
    "            # backpropagation, compute gradients \n",
    "            loss.backward()\n",
    "            \n",
    "            # apply gradients  \n",
    "            optimizer.step()\n",
    "            \n",
    "            current_batch_size = len(x_hat)\n",
    "                      \n",
    "            \n",
    "            if batch_idx % (500//current_batch_size) == 0:\n",
    "                loss, current = loss.item(), batch_idx * current_batch_size\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "                \n",
    "                if plot:\n",
    "                    loss_list.append(loss.item())\n",
    "                \n",
    "                pass\n",
    "        # Save model after each epoch\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, saveModelPath)\n",
    "        print(f\"Checkpoint at epoch {epoch}\")\n",
    "        pass\n",
    "    \n",
    "    if plot:\n",
    "        xVals = list(range(1, len(loss_list) + 1))\n",
    "        \n",
    "        # subplots define number of rows and columns\n",
    "        fig, ax1 = plt.subplots(1, 1)\n",
    "        ax1.plot(xVals, loss_list, 'o-')\n",
    "        fig.suptitle(f\"Loss through training.\")\n",
    "        ax1.set_ylabel(\"Loss over training\")\n",
    "       \n",
    "    print(\"Done!\")            \n",
    "    pass\n",
    "\n",
    "train(startepoch, epochs, model, trainloader, True)\n",
    "# Save final model \n",
    "torch.save(model.state_dict(), saveModelPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to python file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script VAE_CIFAR100_test.ipynb"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d806fedfadc4f43df997bb69e69664c5753a645548adea2a58a56079e3db770c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

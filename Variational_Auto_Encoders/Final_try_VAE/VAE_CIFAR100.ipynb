{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE with the CIFAR100 dataset\n",
    "Training of a VAE on the Cifar100 dataset. VAR inspired by [link](https://github.com/henryqin1997/vae-cifar10/blob/master/cifar10_vae.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import datasets, utils, models\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from ignite.handlers.param_scheduler import create_lr_scheduler_with_warmup \n",
    "import numpy as np\n",
    "from torch import optim\n",
    "import os\n",
    "from os.path import exists\n",
    "\n",
    "\n",
    "## !! For Checkpointing!!!\n",
    "\n",
    "# Path to saving the model\n",
    "save_model_path = \"../../trainedModels/VAE_CIFAR100_final.pth\"\n",
    "save_loss_path = \"../../plottables/VAE_CIFAR100_final.pth\"\n",
    "\n",
    "## WARNING!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# This boolean will completely wipe any past checkpoints or progress.\n",
    "# ASSIGN IT WITH CARE.\n",
    "completelyRestartTrain = True\n",
    "\n",
    "## Important if you want to train again, set this to True\n",
    "tryResumeTrain = True\n",
    "\n",
    "# Inarguably a weird place to initialize the number of epochs\n",
    "# but it is a magic tool that will come in handy later.\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {DEVICE} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msg(\n",
    "    message: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        message (str): a message of type string, which will be printed to the terminal\n",
    "            with some decoration.\n",
    "\n",
    "    Description:\n",
    "        This function takes a message and prints it nicely\n",
    "\n",
    "    Output:\n",
    "        This function has no output, it prints directly to the terminal\n",
    "    \"\"\"\n",
    "\n",
    "    # word_list makes sure that the output of msg is more readable\n",
    "    sentence_list = message.split(sep=\"\\n\")\n",
    "    # the max-function can apparently be utilised like this:\n",
    "    longest_sentence = max(sentence_list, key=len)\n",
    "\n",
    "    n = len(longest_sentence)\n",
    "    n2 = n // 2 - 1\n",
    "    print(\">\" * n2 + \"  \" + \"<\" * n2)\n",
    "    print(message)\n",
    "    print(\">\" * n2 + \"  \" + \"<\" * n2 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ../data/datasetCIFAR100/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "169001984it [00:48, 3507424.57it/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/datasetCIFAR100/cifar-100-python.tar.gz to ../data/datasetCIFAR100\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>  <<<<<<<<<<<<<<<<<<<<<<<<\n",
      "Split train data into trainset and validation set.\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>  <<<<<<<<<<<<<<<<<<<<<<<<\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128 # 32\n",
    "VALIDATION_SPLIT = 0.2\n",
    "RANDOM_SEED = 42\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# Setting seeds ##############################################\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "##############################################################\n",
    "\n",
    "CIFAR100_MEAN = [0.5070751592371323, 0.48654887331495095, 0.4409178433670343]\n",
    "CIFAR100_STD = [0.2673342858792401, 0.2564384629170883, 0.27615047132568404]\n",
    "\n",
    "transform_train = torchvision.transforms.Compose([\n",
    "        #torchvision.transforms.RandomCrop(32, padding=4),\n",
    "        #torchvision.transforms.RandomHorizontalFlip(),\n",
    "        #torchvision.transforms.RandomRotation(15),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        #torchvision.transforms.Normalize(CIFAR100_MEAN, CIFAR100_STD)\n",
    "    ])\n",
    "\n",
    "transform_test = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        #torchvision.transforms.Normalize(CIFAR100_MEAN, CIFAR100_STD)\n",
    "    ])\n",
    "\n",
    "trainval_set = datasets.CIFAR100(\n",
    "    root = '../../data/datasetCIFAR100',\n",
    "    train = True,                         \n",
    "    transform = transform_train, \n",
    "    download = True\n",
    "    )\n",
    "\n",
    "test_set = datasets.CIFAR100(\n",
    "    root = '../../data/datasetCIFAR100', \n",
    "    train = False, \n",
    "    transform = transform_test\n",
    "    )\n",
    "\n",
    "train_num = int(len(trainval_set) * (1 - VALIDATION_SPLIT))\n",
    "train_set, val_set = random_split(trainval_set, [train_num, len(trainval_set) - train_num])\n",
    "msg(\"Split train data into trainset and validation set.\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "\n",
    "classes = trainval_set.classes # or class_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model and train\n",
    "\n",
    "Models from [here](https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py) and VAE structure from here [git](https://github.com/Jackson-Kang/Pytorch-VAE-tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, in_channels, latent_dim, hidden_dims = None, **kwargs) -> None:\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        modules = []\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [32, 64, 128, 256, 512]\n",
    "        \n",
    "        # Encoder\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels= h_dim, \n",
    "                    kernel_size = 3, bias=False, stride = 2, padding = 1 ), \n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU(negative_slope = 0.05, inplace = True),\n",
    "                    nn.Dropout2d(0.2)\n",
    "                )\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        modules = []\n",
    "\n",
    "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1] * 4)\n",
    "\n",
    "        hidden_dims.reverse()\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[i],out_channels= hidden_dims[i + 1],\n",
    "                                       kernel_size=3, stride = 2, padding=1, output_padding=1),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU(negative_slope = 0.05, inplace = True),\n",
    "                    nn.Dropout2d(0.2)\n",
    "                    )\n",
    "            )\n",
    "        \n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "                            nn.ConvTranspose2d(hidden_dims[-1],\n",
    "                                               hidden_dims[-1],\n",
    "                                               kernel_size=3,\n",
    "                                               stride=2,\n",
    "                                               padding=1,\n",
    "                                               output_padding=1),\n",
    "                            nn.BatchNorm2d(hidden_dims[-1]),\n",
    "                            nn.LeakyReLU(),\n",
    "                            nn.Conv2d(hidden_dims[-1], out_channels= 3,\n",
    "                                      kernel_size= 3, stride=2, padding= 1),\n",
    "                            nn.Sigmoid()#nn.Tanh()\n",
    "                            )\n",
    "\n",
    "    def encode(self, input):\n",
    "        result = self.encoder(input)\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "\n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_logvar(result)\n",
    "\n",
    "        return [mu, log_var]\n",
    "        \n",
    "    def decode(self, z):\n",
    "        result = self.decoder_input(z)\n",
    "        result = result.view(-1, 512, 2, 2)\n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        return result\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "        \n",
    "    def forward(self, input, **kwargs):\n",
    "        mu, log_var = self.encode(input)\n",
    "        z = self.reparameterize(mu,  log_var)\n",
    "        return [self.decode(z), mu, log_var]\n",
    "        \n",
    "    def sample(self, num_samples, **kwargs):\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x, **kwargs):\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "         \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model and hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weird classs - warmUp stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "class WarmUpLR(_LRScheduler):\n",
    "    \"\"\"warmup_training learning rate scheduler\n",
    "    Args:\n",
    "        optimizer: optimzier(e.g. SGD)\n",
    "        total_iters: totoal_iters of warmup phase\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, total_iters, last_epoch=-1):\n",
    "\n",
    "        self.total_iters = total_iters\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "        \n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"we will use the first m batches, and set the learning\n",
    "        rate to base_lr * m / total_iters\n",
    "        \"\"\"\n",
    "        return [base_lr * self.last_epoch / (self.total_iters + 1e-8) for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperparameters are:\n",
      ">>>>>>>>>>  <<<<<<<<<<\n",
      "latent space dim: \t200 \n",
      "learning rate \t\t0.0005 \n",
      "Number of epoch \t50 \n",
      "Batch size \t\t128 \n",
      "Weight decay \t\t0.0001\n",
      "Warmup \t\t\t10\n",
      ">>>>>>>>>>  <<<<<<<<<<\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "channel_size = test_set[0][0].shape[0] #Fixed, dim 0 is the feature channel number\n",
    "latent_dim = 200 #From 5 # hyperparameter\n",
    "\n",
    "WARMUP_ITERATIONS = 10\n",
    "WEIGHT_DECAY = 1e-4\n",
    "SGD_MOMENTUM = 0.9\n",
    "INITIAL_LR = 5e-4\n",
    "HIDDEN_DIMS = [32, 64, 64, 64] # Virker måske ikke med færre bloke...\n",
    "numEpochs = 50\n",
    "\n",
    "\n",
    "model = VAE(channel_size,latent_dim).to(DEVICE)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = INITIAL_LR, weight_decay=WEIGHT_DECAY)#optim.SGD(model.parameters(), lr= lr)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=INITIAL_LR, \n",
    "#                            momentum=SGD_MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "iter_per_epoch = len(train_loader)\n",
    "warmup_scheduler = WarmUpLR(optimizer, iter_per_epoch * WARMUP_ITERATIONS)        \n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=numEpochs-WARMUP_ITERATIONS)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"hyperparameters are:\")\n",
    "msg(f\"latent space dim: \\t{latent_dim} \\nlearning rate \\t\\t{INITIAL_LR} \\nNumber of epoch \\t{numEpochs} \\nBatch size \\t\\t{BATCH_SIZE} \\nWeight decay \\t\\t{WEIGHT_DECAY}\\nWarmup \\t\\t\\t{WARMUP_ITERATIONS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DimCheck = False\n",
    "\n",
    "if DimCheck:\n",
    "    x = torch.randn(2,3,32,32)\n",
    "    print(f\"Size of input {x.size()}\")\n",
    "    # Encoder test\n",
    "    x_hat, mu, log_var = model(x)\n",
    "    print(f\"The mean shape {mu.size()}, \\tthe variance shape {log_var.size()}\")\n",
    "\n",
    "    print(f\"x_hat {x_hat.size()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpointing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>>>>>>  <<<<<<<<<<<<<<<<<<<\n",
      "There was no previously existing model. \n",
      "Training will commence from beginning.\n",
      ">>>>>>>>>>>>>>>>>>>  <<<<<<<<<<<<<<<<<<<\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# It is important that it is initialized to zero\n",
    "# if we are in the case that a model hasn't been trained yet.\n",
    "loss_train = np.zeros((2, numEpochs))\n",
    "loss_val = np.zeros((2, numEpochs))\n",
    "            \n",
    "# exists is a function from os.path (standard library)\n",
    "trained_model_exists = exists(save_model_path)\n",
    "\n",
    "if trained_model_exists:\n",
    "    if completelyRestartTrain:\n",
    "        os.remove(save_model_path)\n",
    "        startEpoch = 0\n",
    "        msg(\"Previous model was deleted. \\nRestarting training.\")\n",
    "    else:\n",
    "        import collections\n",
    "        if not (type(torch.load(save_model_path)) is collections.OrderedDict):\n",
    "            ## If it looks stupid but works it ain't stupid B)\n",
    "            #\n",
    "            # I think if it isn't that datatype, then it saved the Alex-way\n",
    "            # and then we can load stuff.\n",
    "            # Because if it is that datatype then it is for sure \"just\" the state_dict.\n",
    "            \n",
    "            checkpoint = torch.load(save_model_path)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            \n",
    "            num_previous_epochs = checkpoint['accuracies'].shape[1]\n",
    "            \n",
    "            if num_previous_epochs < numEpochs:\n",
    "                # if we trained to fewer epochs previously,\n",
    "                # now we train to the proper amount, therfore padded zeroes are required.\n",
    "                remainingZeros = np.zeros((2,numEpochs-num_previous_epochs))\n",
    "                checkpoint['accuracies'] = np.concatenate((checkpoint['accuracies'], remainingZeros), axis=1)\n",
    "                checkpoint['losses'] = np.concatenate((checkpoint['losses'], remainingZeros), axis=1)\n",
    "                \n",
    "            if numEpochs < num_previous_epochs:\n",
    "                # Just cut off our data at the required amount of epochs, so nothing looks funky.\n",
    "                checkpoint['accuracies'] = checkpoint['accuracies'][:,:numEpochs]\n",
    "                checkpoint['losses'] = checkpoint['losses'][:,:numEpochs]\n",
    "            \n",
    "            # we save at the epoch we completed, but we wan't to start at the following epoch\n",
    "            startEpoch = checkpoint['epoch'] + 1 \n",
    "            \n",
    "            if startEpoch < numEpochs:\n",
    "                # we add one to startEpoch here (in the message) to make up for\n",
    "                # the for-loop being zero-indexed.\n",
    "                msg(f\"Model will resume training from epoch: {startEpoch + 1}\")\n",
    "                \n",
    "                # grapping our accuracies from the previously trained model\n",
    "                accuracies = checkpoint['accuracies']\n",
    "                losses = checkpoint['losses']\n",
    "                \n",
    "            elif tryResumeTrain and startEpoch >= numEpochs:\n",
    "                msg(\"Model has already finished training. \"\n",
    "                    + \"\\nDo you wan't to delete previous model and start over?\")\n",
    "                userInput = input(\"Input [y/n]:\\t\")\n",
    "                while userInput.lower() != 'y' and userInput.lower != 'n':\n",
    "                    userInput = input(\"You must input either 'y' (yes) or 'n' (no):\\t\")\n",
    "                if userInput.lower() == \"y\":\n",
    "                    os.remove(save_model_path)\n",
    "                    startEpoch = 0\n",
    "                    msg(\"Previous model was deleted. \\nRestarting training!\")\n",
    "                elif userInput.lower() == \"n\":\n",
    "                    msg(\"Model had already finished training and no new training will commence.\")\n",
    "                    \n",
    "            elif not tryResumeTrain and startEpoch >= numEpochs:\n",
    "                msg(f\"Model finished training at epoch: {startEpoch}\")\n",
    "                # grapping our accuracies from the previously trained model\n",
    "                accuracies = checkpoint['accuracies']\n",
    "                losses = checkpoint['losses']\n",
    "                            \n",
    "else:\n",
    "    #Trained model doesn't exist\n",
    "    msg(\"There was no previously existing model. \\nTraining will commence from beginning.\")\n",
    "    startEpoch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "In CIFAR100. First define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_function(x, x_hat, mean, log_var):\n",
    "    reproduction_loss = nn.MSELoss()(x_hat, x) #F.l1_loss(x_hat, x) \n",
    "    #KLD      = - 0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mean**2 - torch.exp(log_var), axis=1)#torch.mean( -0.5 * torch.sum(1+ log_var - mean**2 - log_var.exp(),dim=1),dim = 0) # Mean loss for the whole batch\n",
    "    KLD = KLD.mean() \n",
    "    #KLD *= 0.00025\n",
    "    \n",
    "    #print(f\"Reproduction: {reproduction_loss}, \\tKLD: {KLD.item()}, \\tscaled KLD: {(KLD * scale).item()}, \\tlog_var: {log_var.sum()}\")\n",
    "    return reproduction_loss, KLD "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and testing loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    size = len(loader.dataset)\n",
    "    train_avg_KLD = 0\n",
    "    train_avg_repo = 0\n",
    "    num_batches = len(loader)\n",
    "\n",
    "    for batch_idx, (x, _) in enumerate(loader):\n",
    "        \n",
    "        x = x.to(DEVICE)\n",
    "\n",
    "        # Model pred\n",
    "        x_hat, mean, log_var = model(x)\n",
    "\n",
    "        # Compute loss\n",
    "        loss_repo, loss_KLD = loss_fn(x, x_hat, mean, log_var)\n",
    "        loss = loss_repo + loss_KLD\n",
    "\n",
    "        train_avg_repo += loss_repo.item()\n",
    "        train_avg_KLD += loss_KLD.item()\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        current_batch_size = len(x)\n",
    "        \n",
    "        # Check gradient and loss\n",
    "        if (batch_idx + 1) % (5000//current_batch_size) == 0 or batch_idx == 0:\n",
    "            # Print loss\n",
    "            loss, current = loss.item(), batch_idx * current_batch_size\n",
    "            print(f\"loss: repo: {loss_repo.item() :>4f}\\t KLD scaled: {loss_KLD.item() :>4f}  [{current+1:>5d}/{size:>5d}]\")\n",
    "\n",
    "        if epoch <= WARMUP_ITERATIONS:\n",
    "            warmup_scheduler.step()\n",
    "        \n",
    "    train_avg_repo /= num_batches\n",
    "    train_avg_KLD /= num_batches\n",
    "\n",
    "    return train_avg_repo, train_avg_KLD\n",
    "\n",
    "def test_loop(model, loader, loss_fn):\n",
    "    model.eval()\n",
    "\n",
    "    num_batches = len(loader)\n",
    "    val_avg_KLD = 0\n",
    "    val_avg_repo = 0\n",
    "    with torch.no_grad():\n",
    "        for (x,_) in loader:\n",
    "            # Get data\n",
    "            x = x.to(DEVICE)\n",
    "\n",
    "            # Compute loss\n",
    "            x_hat, mean, log_var = model(x)\n",
    "            loss_repo, loss_KLD = loss_fn(x, x_hat, mean, log_var)\n",
    "\n",
    "            val_avg_repo += loss_repo.item()\n",
    "            val_avg_KLD += loss_KLD.item()\n",
    "\n",
    "    val_avg_repo /= num_batches\n",
    "    val_avg_KLD /= num_batches\n",
    "    return val_avg_repo, val_avg_KLD \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the training begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>  <<<<<<<<<<<\n",
      "Will now begin training!\n",
      ">>>>>>>>>>>  <<<<<<<<<<<\n",
      "\n",
      "Epoch 1\n",
      "----------------------------------\n",
      "loss: repo: 0.081560\t KLD scaled: 32.117207  [    1/40000]\n",
      "loss: repo: 0.079396\t KLD scaled: 25.650871  [ 4865/40000]\n",
      "loss: repo: 0.076416\t KLD scaled: 23.584600  [ 9857/40000]\n"
     ]
    }
   ],
   "source": [
    "if not trained_model_exists or tryResumeTrain or startEpoch < (numEpochs - 1):\n",
    "    best_loss = np.inf\n",
    "\n",
    "    msg(\"Will now begin training!\")\n",
    "    for epoch in range(startEpoch,numEpochs):\n",
    "        if epoch > WARMUP_ITERATIONS:\n",
    "            # TODO make sure it matches scheduler\n",
    "            scheduler.step()\n",
    "        print(f\"Epoch {epoch +1}\\n----------------------------------\")\n",
    "        train_avg_repo, train_avg_KLD   = train_loop(model, train_loader, loss_function, optimizer,)\n",
    "        val_avg_repo, val_avg_KLD       = test_loop(model, val_loader, loss_function,)\n",
    "\n",
    "        # Save information for plotting\n",
    "        loss_train[0,epoch], loss_train[1,epoch]    = train_avg_repo, train_avg_KLD\n",
    "        loss_val[0, epoch], loss_val[1, epoch]      = val_avg_repo, val_avg_KLD     \n",
    "        avg_loss_val = val_avg_repo + val_avg_KLD\n",
    "\n",
    "        if avg_loss_val < best_loss:\n",
    "            # We only save a checkpoint if our model is performing better\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, save_model_path)\n",
    "            best_loss = avg_loss_val\n",
    "\n",
    "            msg(f\"New best loss is: {avg_loss_val} \\nCheckpoint at epoch: {epoch + 1}\")\n",
    "        else:\n",
    "            msg(\"Only test and val losses were updated\")\n",
    "        \n",
    "        torch.save({\"train_loss\" : loss_train, \"val_loss\" : loss_val}, save_loss_path)\n",
    "            \n",
    "    msg(f\"Done! Final model was saved to: \\n'{save_model_path}'\")\n",
    "    \n",
    "else:\n",
    "    msg(\"Have already trained this model once!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot reproduction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_path = \"VAE_CIFAR100_mix4.pth\"  #\"VAE_CIFAR100_4.pth\"#\"VAE_CIFAR100_linVGG16.pth-kopi\"#/Users/Alex/Documents/results/savedModel/VAE_CIFAR100_linVGG16.pth\"#\"VAE_CIFAR100.pth\"\n",
    "checkpoint = torch.load(my_model_path, map_location=torch.device(DEVICE))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (0) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-46546b662133>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mbatch_show\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mbatchplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_show\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-46546b662133>\u001b[0m in \u001b[0;36mbatchplot\u001b[0;34m(batch_show, image)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_show\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvert_normalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \"\"\"\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m     \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (0) at non-singleton dimension 0"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1224x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# invert_normalization = torchvision.transforms.Compose([\n",
    "#          torchvision.transforms.Normalize(mean = [0, 0, 0], std = 1/CIFAR100_STD),\n",
    "#          torchvision.transforms.Normalize(mean = -1*CIFAR100_MEAN, std = [1,1,1])])\n",
    "\n",
    "# Set the model in evaluation mode. In this case this is for the Dropout layers\n",
    "model.eval()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "model.eval()\n",
    "\n",
    "def batchplot(batch_show,image):\n",
    "# How many images from the batch will you show?\n",
    "    invert_normalization = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Normalize(mean = [0, 0, 0], std = [1/0.5070751592371323, 1/0.48654887331495095, 1/0.4409178433670343]),\n",
    "        torchvision.transforms.Normalize(mean = -1*[0.2673342858792401, 0.2564384629170883, 0.27615047132568404], std = [1,1,1])\n",
    "    ])\n",
    "\n",
    "    def imshow(img):\n",
    "        #img = img / 2 + 0.5     # unnormalize\n",
    "            npimg = img.numpy()\n",
    "            plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "            #plt.show()\n",
    "\n",
    "    image\n",
    "    # Model reconstruction\n",
    "    with torch.no_grad():\n",
    "        x_hat, mean, logvar = model(image)\n",
    "        z = mean + torch.exp(0.5*logvar)\n",
    "        x_hat = model.decode(z)\n",
    "\n",
    "    #print(image.size())\n",
    "    #image = invert_normalization(image)\n",
    "    #x_hat = invert_normalization(x_hat)\n",
    "    #x.transform(invert_normalization)\n",
    "\n",
    "    fig1=plt.figure(figsize=(17,4))\n",
    "    fig1.patch.set_facecolor('white')\n",
    "    for i in range(batch_show):\n",
    "\n",
    "        plt.subplot(2,batch_show,i+1)\n",
    "        imshow(invert_normalization(image[i]))\n",
    "        plt.xticks([],[])\n",
    "        plt.yticks([],[])\n",
    "        plt.title(classes[labels[i].item()])\n",
    "        if i == 0:\n",
    "            plt.ylabel('Original image')\n",
    "        \n",
    "        plt.subplot(2,batch_show,batch_show+ i+1)\n",
    "        imshow(invert_normalization(x_hat[i]))\n",
    "        plt.xticks([],[])\n",
    "        plt.yticks([],[])\n",
    "        if i == 0:\n",
    "            plt.ylabel('Reproduced image')\n",
    "    pass\n",
    "\n",
    "\n",
    "dataiter = iter(test_loader)\n",
    "x, labels = dataiter.next()\n",
    "\n",
    "batch_show = 7\n",
    "\n",
    "batchplot(batch_show,x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to python file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook VAE_CIFAR100_mix.ipynb to script\n",
      "[NbConvertApp] Writing 20533 bytes to VAE_CIFAR100_mix.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script VAE_CIFAR100_mix.ipynb"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d806fedfadc4f43df997bb69e69664c5753a645548adea2a58a56079e3db770c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

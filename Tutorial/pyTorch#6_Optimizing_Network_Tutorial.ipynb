{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIMIZING MODEL PARAMETERS\n",
    "\n",
    "This [tutorial](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html) is also, also, also given a webpage.\n",
    "\n",
    "This will mostly just be another example at a nerual network.\n",
    "\n",
    "I wrote some (very sparse) notes on optimizers in Overleaf. I think that the main take-away is that Adam is the best optimizer most of the time.\n",
    "But SGD (Stochastic Gradient Descent) might be faster sometimes. It atleast doesn't require as much computational power.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensue a lot of code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Hyperparameters\n",
    "\n",
    "I wrote a great big note on this as well.\n",
    "\n",
    "Short explanation:\n",
    "They are parameters that, by tuning them, lets you optimize the way the model optimizes. The following three are (propably) the most common of hyperparemters.\n",
    "\n",
    "* **Epochs** the numer of times that the model goes through all of the training data in the learning process.\n",
    "* **Batch Size** the number of observations (datapoints), also the size of the batch, that goes through the newtork before the weights are updated.\n",
    "* **Learning Rate** how much the weights are updated after each batch/ epoch.\n",
    "Smaller learnings rates are slow, but high learning rates might divergce from a global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "\n",
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# We pick the SGD optimizer for now.\n",
    "# But here we actually tried the Adam optimizer with great success.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test loop\n",
    "\n",
    "Then the code for the training and the testing loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing it all\n",
    "\n",
    "Finally the code to actually execute the model, and train- and test loop we wrote:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.298959  [    0/60000]\n",
      "loss: 0.577881  [ 6400/60000]\n",
      "loss: 0.398128  [12800/60000]\n",
      "loss: 0.483586  [19200/60000]\n",
      "loss: 0.460194  [25600/60000]\n",
      "loss: 0.446704  [32000/60000]\n",
      "loss: 0.384780  [38400/60000]\n",
      "loss: 0.506169  [44800/60000]\n",
      "loss: 0.505709  [51200/60000]\n",
      "loss: 0.500542  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.428353 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.270128  [    0/60000]\n",
      "loss: 0.364951  [ 6400/60000]\n",
      "loss: 0.281438  [12800/60000]\n",
      "loss: 0.403278  [19200/60000]\n",
      "loss: 0.395632  [25600/60000]\n",
      "loss: 0.379442  [32000/60000]\n",
      "loss: 0.324930  [38400/60000]\n",
      "loss: 0.474144  [44800/60000]\n",
      "loss: 0.440202  [51200/60000]\n",
      "loss: 0.457556  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.4%, Avg loss: 0.395458 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.210494  [    0/60000]\n",
      "loss: 0.333922  [ 6400/60000]\n",
      "loss: 0.227882  [12800/60000]\n",
      "loss: 0.331185  [19200/60000]\n",
      "loss: 0.372771  [25600/60000]\n",
      "loss: 0.352553  [32000/60000]\n",
      "loss: 0.301001  [38400/60000]\n",
      "loss: 0.420810  [44800/60000]\n",
      "loss: 0.345486  [51200/60000]\n",
      "loss: 0.367641  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.382693 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.204749  [    0/60000]\n",
      "loss: 0.289841  [ 6400/60000]\n",
      "loss: 0.196121  [12800/60000]\n",
      "loss: 0.300403  [19200/60000]\n",
      "loss: 0.342587  [25600/60000]\n",
      "loss: 0.317111  [32000/60000]\n",
      "loss: 0.263082  [38400/60000]\n",
      "loss: 0.391714  [44800/60000]\n",
      "loss: 0.311842  [51200/60000]\n",
      "loss: 0.379345  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.375727 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.204797  [    0/60000]\n",
      "loss: 0.264612  [ 6400/60000]\n",
      "loss: 0.200228  [12800/60000]\n",
      "loss: 0.254089  [19200/60000]\n",
      "loss: 0.350091  [25600/60000]\n",
      "loss: 0.321558  [32000/60000]\n",
      "loss: 0.277500  [38400/60000]\n",
      "loss: 0.371482  [44800/60000]\n",
      "loss: 0.298457  [51200/60000]\n",
      "loss: 0.323930  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.351606 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.176795  [    0/60000]\n",
      "loss: 0.233413  [ 6400/60000]\n",
      "loss: 0.178468  [12800/60000]\n",
      "loss: 0.243471  [19200/60000]\n",
      "loss: 0.320213  [25600/60000]\n",
      "loss: 0.269606  [32000/60000]\n",
      "loss: 0.227819  [38400/60000]\n",
      "loss: 0.333273  [44800/60000]\n",
      "loss: 0.234907  [51200/60000]\n",
      "loss: 0.305474  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.360897 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.164522  [    0/60000]\n",
      "loss: 0.239221  [ 6400/60000]\n",
      "loss: 0.151277  [12800/60000]\n",
      "loss: 0.215116  [19200/60000]\n",
      "loss: 0.277155  [25600/60000]\n",
      "loss: 0.268987  [32000/60000]\n",
      "loss: 0.216701  [38400/60000]\n",
      "loss: 0.346344  [44800/60000]\n",
      "loss: 0.259106  [51200/60000]\n",
      "loss: 0.297127  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.7%, Avg loss: 0.364080 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.159002  [    0/60000]\n",
      "loss: 0.208760  [ 6400/60000]\n",
      "loss: 0.166383  [12800/60000]\n",
      "loss: 0.212959  [19200/60000]\n",
      "loss: 0.283181  [25600/60000]\n",
      "loss: 0.271358  [32000/60000]\n",
      "loss: 0.230480  [38400/60000]\n",
      "loss: 0.291673  [44800/60000]\n",
      "loss: 0.276276  [51200/60000]\n",
      "loss: 0.290320  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.372244 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.150952  [    0/60000]\n",
      "loss: 0.184212  [ 6400/60000]\n",
      "loss: 0.173814  [12800/60000]\n",
      "loss: 0.208690  [19200/60000]\n",
      "loss: 0.273218  [25600/60000]\n",
      "loss: 0.265876  [32000/60000]\n",
      "loss: 0.202471  [38400/60000]\n",
      "loss: 0.272378  [44800/60000]\n",
      "loss: 0.267407  [51200/60000]\n",
      "loss: 0.290316  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.347940 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.168375  [    0/60000]\n",
      "loss: 0.168989  [ 6400/60000]\n",
      "loss: 0.167368  [12800/60000]\n",
      "loss: 0.227491  [19200/60000]\n",
      "loss: 0.224976  [25600/60000]\n",
      "loss: 0.233174  [32000/60000]\n",
      "loss: 0.174290  [38400/60000]\n",
      "loss: 0.316120  [44800/60000]\n",
      "loss: 0.207318  [51200/60000]\n",
      "loss: 0.252572  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.9%, Avg loss: 0.372296 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.150291  [    0/60000]\n",
      "loss: 0.135564  [ 6400/60000]\n",
      "loss: 0.186980  [12800/60000]\n",
      "loss: 0.216566  [19200/60000]\n",
      "loss: 0.233229  [25600/60000]\n",
      "loss: 0.221757  [32000/60000]\n",
      "loss: 0.129152  [38400/60000]\n",
      "loss: 0.231802  [44800/60000]\n",
      "loss: 0.184896  [51200/60000]\n",
      "loss: 0.244687  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.386467 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.113402  [    0/60000]\n",
      "loss: 0.129181  [ 6400/60000]\n",
      "loss: 0.199621  [12800/60000]\n",
      "loss: 0.177440  [19200/60000]\n",
      "loss: 0.311557  [25600/60000]\n",
      "loss: 0.272643  [32000/60000]\n",
      "loss: 0.135206  [38400/60000]\n",
      "loss: 0.228770  [44800/60000]\n",
      "loss: 0.247970  [51200/60000]\n",
      "loss: 0.287328  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.8%, Avg loss: 0.397063 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.117957  [    0/60000]\n",
      "loss: 0.157113  [ 6400/60000]\n",
      "loss: 0.175154  [12800/60000]\n",
      "loss: 0.188607  [19200/60000]\n",
      "loss: 0.206739  [25600/60000]\n",
      "loss: 0.271892  [32000/60000]\n",
      "loss: 0.133847  [38400/60000]\n",
      "loss: 0.187260  [44800/60000]\n",
      "loss: 0.131856  [51200/60000]\n",
      "loss: 0.178283  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.8%, Avg loss: 0.412509 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.118985  [    0/60000]\n",
      "loss: 0.134634  [ 6400/60000]\n",
      "loss: 0.206224  [12800/60000]\n",
      "loss: 0.195571  [19200/60000]\n",
      "loss: 0.271407  [25600/60000]\n",
      "loss: 0.265183  [32000/60000]\n",
      "loss: 0.149024  [38400/60000]\n",
      "loss: 0.226272  [44800/60000]\n",
      "loss: 0.170693  [51200/60000]\n",
      "loss: 0.252514  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.400645 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.099307  [    0/60000]\n",
      "loss: 0.099519  [ 6400/60000]\n",
      "loss: 0.206553  [12800/60000]\n",
      "loss: 0.187401  [19200/60000]\n",
      "loss: 0.218469  [25600/60000]\n",
      "loss: 0.293069  [32000/60000]\n",
      "loss: 0.121358  [38400/60000]\n",
      "loss: 0.209241  [44800/60000]\n",
      "loss: 0.113823  [51200/60000]\n",
      "loss: 0.147481  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.420762 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.184225  [    0/60000]\n",
      "loss: 0.096291  [ 6400/60000]\n",
      "loss: 0.188866  [12800/60000]\n",
      "loss: 0.220961  [19200/60000]\n",
      "loss: 0.193369  [25600/60000]\n",
      "loss: 0.317183  [32000/60000]\n",
      "loss: 0.196444  [38400/60000]\n",
      "loss: 0.200002  [44800/60000]\n",
      "loss: 0.089007  [51200/60000]\n",
      "loss: 0.162830  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.435132 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.102803  [    0/60000]\n",
      "loss: 0.135485  [ 6400/60000]\n",
      "loss: 0.178885  [12800/60000]\n",
      "loss: 0.186692  [19200/60000]\n",
      "loss: 0.156388  [25600/60000]\n",
      "loss: 0.238325  [32000/60000]\n",
      "loss: 0.090738  [38400/60000]\n",
      "loss: 0.179443  [44800/60000]\n",
      "loss: 0.127057  [51200/60000]\n",
      "loss: 0.133616  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.432894 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.081332  [    0/60000]\n",
      "loss: 0.139980  [ 6400/60000]\n",
      "loss: 0.162622  [12800/60000]\n",
      "loss: 0.139279  [19200/60000]\n",
      "loss: 0.242557  [25600/60000]\n",
      "loss: 0.201875  [32000/60000]\n",
      "loss: 0.050551  [38400/60000]\n",
      "loss: 0.146218  [44800/60000]\n",
      "loss: 0.099241  [51200/60000]\n",
      "loss: 0.139665  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.445699 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.090201  [    0/60000]\n",
      "loss: 0.107914  [ 6400/60000]\n",
      "loss: 0.156438  [12800/60000]\n",
      "loss: 0.143567  [19200/60000]\n",
      "loss: 0.308014  [25600/60000]\n",
      "loss: 0.278885  [32000/60000]\n",
      "loss: 0.119106  [38400/60000]\n",
      "loss: 0.152723  [44800/60000]\n",
      "loss: 0.107086  [51200/60000]\n",
      "loss: 0.107134  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.466175 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.093652  [    0/60000]\n",
      "loss: 0.124744  [ 6400/60000]\n",
      "loss: 0.168546  [12800/60000]\n",
      "loss: 0.142306  [19200/60000]\n",
      "loss: 0.172388  [25600/60000]\n",
      "loss: 0.239691  [32000/60000]\n",
      "loss: 0.076177  [38400/60000]\n",
      "loss: 0.162702  [44800/60000]\n",
      "loss: 0.065031  [51200/60000]\n",
      "loss: 0.125234  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.494120 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model\n",
    "Just so it does not have to be trained each time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the model!\n",
    "\n",
    "Yeah... what it says. Lets evaluate the model on the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 'Coat', Actual: 'Shirt'\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Defining the classes for \n",
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# The data is formatted as a matrix (so to speak), \n",
    "# it is 10.000 observations with their labels.\n",
    "# So at the first row, the first column is this big 28 x 28 tensor image,\n",
    "# and the second column at the first row is it's corresponding label.\n",
    "N_DataPoints = len(test_data)\n",
    "\n",
    "sample_idx = torch.randint(N_DataPoints, size=(1,)).item()\n",
    "x, y = test_data[sample_idx][0], test_data[sample_idx][1]\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f\"Predicted: '{predicted}', Actual: '{actual}'\")\n",
    "\n",
    "# print(f\"Random number: {sample_idx} \\nNumber o datapoints: {N_DataPoints}\")\n",
    "tensor_im = test_data[sample_idx][0]\n",
    "reshaped_im = tensor_im.view(tensor_im.shape[1], tensor_im.shape[2], tensor_im.shape[0])\n",
    "\n",
    "# Actually showing the image:\n",
    "plt.imshow(reshaped_im, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-23.8287,  37.9768, -33.8225, -20.7378, -30.7520, -58.0915, -20.0393,\n",
      "         -78.8957, -26.9773, -54.2201]])\n",
      "tensor(1)\n",
      "tensor(3)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(pred)\n",
    "print(pred[0].argmax())\n",
    "a = pred[0].argmax()\n",
    "print(a + 2)\n",
    "bla = list(range(10))\n",
    "print(bla[a + 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "It seemed like depth of the Network didn't matter much. And the learning rate just slowed it down, it still achieved the same accuracy.\n",
    "\n",
    "For this example, at least, it seemed like it was the amount of epochs which were critical. \n",
    "For further development we should probably have a look at Bayesian-optimization."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c543ace77a335d6d53f78c966f120f62d739252c040c86805b5b43f8832d8b96"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('Bachelor_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "\n",
    "The explanation of tensors through the [tutorial](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html), states that they are similair to matrices and arrays.\n",
    "However they can run on GPU's, which significantly speeds up the computation process of machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting started with the big imports\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing a Tensor\n",
    "\n",
    "Different ways to initialize a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensors: \n",
      " tensor([[0.4462, 0.3486],\n",
      "        [0.5292, 0.4667]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.7362, 0.8798, 0.5979],\n",
      "        [0.7320, 0.5044, 0.0348]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tensors (like numpy arrays) can be created directly from data.\n",
    "# the data type is automatically infered (which is pretty STONKS)\n",
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "\n",
    "\n",
    "## From NUMPY arrays\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "\n",
    "\n",
    "## From another Tensor\n",
    "# and the new tensor retains both the datatype and shape of the previous tensor\n",
    "x_ones = torch.ones_like(x_data) # retains the properties of the x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype = torch.float) # overrides the datatype\n",
    "# of x_data\n",
    "print(f\"Random Tensors: \\n {x_rand} \\n\")\n",
    "\n",
    "\n",
    "## But also with random or constant values:\n",
    "# Here these different Tensors has a set shape:\n",
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"zeros Tensor: \\n {zeros_tensor} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atributes of a Tensor\n",
    "Here are some of the attributes of a Tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Tensor: torch.Size([3, 4])\n",
      "Datatype of the Tensor: torch.float32\n",
      "Device Tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of the Tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of the Tensor: {tensor.dtype}\")\n",
    "print(f\"Device Tensor is stored on: {tensor.device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations on Tensors\n",
    "\n",
    "[Here](https://pytorch.org/docs/stable/torch.html) is the online documentation for the operations that can be performed on and with Tensors.\n",
    "There are quite a lot, so it might be a good idea to have it saved.\n",
    "\n",
    "Since all these operations can be run on the GPU (and usually much faster there), we would like to move our tensors to the GPU. This has to be done manually though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We move our tensors to the GPU if avaliable\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to(\"cuda\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Tensor API is very similair to how NumPy works.\n",
    "\n",
    "Here are some example operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row: tensor([1., 1., 1., 1.])\n",
      "First column: tensor([1., 1., 1., 1.])\n",
      "Last column: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 1., 1., 0.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [1., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4,4)\n",
    "print(f\"First row: {tensor[0]}\")\n",
    "print(f\"First column: {tensor[:,0]}\")\n",
    "print(f\"Last column: {tensor[..., -1]}\")     # This particular syntax was a bit weird to me..\n",
    "\n",
    "tensor[:,-1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining (concatanation)\n",
    "Furthermore, Tensors can be joined by concatanation (I think concatanation might just be a different word for joining).\n",
    "\n",
    "Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim = 1)\n",
    "print(t1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arithmetic Operations\n",
    "\n",
    "Different arithmetic operations with Tensors (very informative I know)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 0.],\n",
       "        [1., 1., 1., 0.],\n",
       "        [1., 1., 1., 0.],\n",
       "        [1., 1., 1., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "\n",
    "y3 = torch.rand_like(tensor)\n",
    "torch.matmul(tensor, tensor.T, out=y3)\n",
    "\n",
    "\n",
    "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-element tensors \n",
    "\n",
    "If you have a single value Tensor (one-element tensor), it can be converted into a Python numerical value using the `item()` operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Place operations\n",
    "\n",
    "These are operations that overwrite the current object (tensor), instead of creating a new instance.\n",
    "These operators are denoted by the `_` suffix.\n",
    "Examples from the tutorial are:\n",
    "`x.copy_(y)` and `x.t_()`, which will both change the value of `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 0.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [1., 1., 1., 0.]]) \n",
      "\n",
      "tensor([[6., 6., 6., 5.],\n",
      "        [6., 6., 6., 5.],\n",
      "        [6., 6., 6., 5.],\n",
      "        [6., 6., 6., 5.]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{tensor} \\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bridge with NumPy\n",
    "\n",
    "Tensors on the CPU (I do not know whether this is restricted to the Tensors on the CPU or whether it is also a possibility for the Tensors on the GPU) and NumPy arrays can share their underlying memory locations, and changing one will change the other.\n",
    "\n",
    "This is the problem/ nice attribute of having copies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n",
      "\n",
      "After addition of 1:\n",
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")\n",
    "\n",
    "\n",
    "# A change in the Tensor is also reflected in the NumPy array\n",
    "t.add_(1)\n",
    "print(\"\\nAfter addition of 1:\")\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NumPy array to Tensor\n",
    "\n",
    "It is the same the other way around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "n: [1. 1. 1. 1. 1.]\n",
      "\n",
      "After addition of 1:\n",
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")\n",
    "\n",
    "# And a change in one is still reflected in the other:\n",
    "np.add(n, 1, out=n)\n",
    "print(\"\\nAfter addition of 1:\")\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c543ace77a335d6d53f78c966f120f62d739252c040c86805b5b43f8832d8b96"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('Bachelor_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

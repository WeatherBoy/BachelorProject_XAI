{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# These are the humble beginnings for pyTorch\n",
    "The following [tutorial](https://pytorch.org/tutorials/beginner/basics/intro.html) has been used!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# This is just the great import of all the libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets \n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "\n",
    "# Then we download training data, which my computer is probably gonna find highly\n",
    "# irresponsible\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root = \"data\",\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = ToTensor(),\n",
    ")\n",
    "\n",
    "# And also downloading the test data\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root = \"data\",\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = ToTensor(),\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Then we create something called data loaders:\n",
    "train_dataloader = DataLoader(training_data, batch_size = batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size = batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Models\n",
    "So when we create a neural network in pyTorch it is apparently just creating a class which inherits from\n",
    "the nn.Module class.\n",
    "If classes are a little unfamiliar check out this great [link](https://www.w3schools.com/python/python_classes.asp). If inheritence is also a little unfamiliar,\n",
    "this [link](https://www.w3schools.com/python/python_inheritance.asp) does wonders!\n",
    "So here is an example of defining a neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# I am uncertain how this exactly works, and it should probably be homework!\n",
    "# Get CPU or GPU for training\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the Model Parameters\n",
    "\n",
    "When we train a model we obviously need a loss function and an optimizer.\n",
    "\n",
    "Which is what we will be looking at here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.299400  [    0/60000]\n",
      "loss: 2.286723  [ 6400/60000]\n",
      "loss: 2.269109  [12800/60000]\n",
      "loss: 2.275484  [19200/60000]\n",
      "loss: 2.245312  [25600/60000]\n",
      "loss: 2.228361  [32000/60000]\n",
      "loss: 2.242835  [38400/60000]\n",
      "loss: 2.204467  [44800/60000]\n",
      "loss: 2.208388  [51200/60000]\n",
      "loss: 2.187904  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 2.170998 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.173972  [    0/60000]\n",
      "loss: 2.166065  [ 6400/60000]\n",
      "loss: 2.114031  [12800/60000]\n",
      "loss: 2.139895  [19200/60000]\n",
      "loss: 2.086643  [25600/60000]\n",
      "loss: 2.033622  [32000/60000]\n",
      "loss: 2.072351  [38400/60000]\n",
      "loss: 1.990450  [44800/60000]\n",
      "loss: 2.007109  [51200/60000]\n",
      "loss: 1.942393  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.2%, Avg loss: 1.929757 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.955655  [    0/60000]\n",
      "loss: 1.924351  [ 6400/60000]\n",
      "loss: 1.816357  [12800/60000]\n",
      "loss: 1.861954  [19200/60000]\n",
      "loss: 1.757305  [25600/60000]\n",
      "loss: 1.704592  [32000/60000]\n",
      "loss: 1.735399  [38400/60000]\n",
      "loss: 1.631258  [44800/60000]\n",
      "loss: 1.659768  [51200/60000]\n",
      "loss: 1.557096  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 1.565143 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.626990  [    0/60000]\n",
      "loss: 1.585633  [ 6400/60000]\n",
      "loss: 1.442282  [12800/60000]\n",
      "loss: 1.509729  [19200/60000]\n",
      "loss: 1.399497  [25600/60000]\n",
      "loss: 1.392633  [32000/60000]\n",
      "loss: 1.404639  [38400/60000]\n",
      "loss: 1.328529  [44800/60000]\n",
      "loss: 1.364207  [51200/60000]\n",
      "loss: 1.261757  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 1.283200 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.358294  [    0/60000]\n",
      "loss: 1.334777  [ 6400/60000]\n",
      "loss: 1.174457  [12800/60000]\n",
      "loss: 1.272676  [19200/60000]\n",
      "loss: 1.156048  [25600/60000]\n",
      "loss: 1.184695  [32000/60000]\n",
      "loss: 1.197564  [38400/60000]\n",
      "loss: 1.137479  [44800/60000]\n",
      "loss: 1.176864  [51200/60000]\n",
      "loss: 1.087828  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.106760 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# lr stands for learning rate\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "# In one instance of a training loop our network (the model) makes predictions\n",
    "# based on the trainning dataset (fed to the model in batches),\n",
    "# then it backpropagates the prediction error to adjust the model parameters (those we optimise)\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "# To ensure that the model is actually learning something, we test its performance\n",
    "# against the test dataset.\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "\n",
    "# Finally, the training process is conducted over several epochs (runs through the training set)\n",
    "# We print the models loss and accuracy at each epoch, to show whether the procedure\n",
    "# actually advances\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving models\n",
    "\n",
    "Something about saving the model (very informative... I know)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading models\n",
    "\n",
    "Saving and loading models seems like two sides of the same coin... right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 'Ankle boot', Actual: 'Ankle boot'\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "# The loaded model can then be used to make predictions:\n",
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f\"Predicted: '{predicted}', Actual: '{actual}'\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c543ace77a335d6d53f78c966f120f62d739252c040c86805b5b43f8832d8b96"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('Bachelor_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

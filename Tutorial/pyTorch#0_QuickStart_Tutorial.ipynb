{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# These are the humble beginnings for pyTorch\n",
    "The following [tutorial](https://pytorch.org/tutorials/beginner/basics/intro.html) has been used!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# This is just the great import of all the libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets \n",
    "from torchvision.transforms import ToTensor\n",
    "from os.path import exists\n",
    "\n",
    "# Path for where we save the model\n",
    "# this is a magic tool that will come in handy later ;)\n",
    "saveModelPath = \"../trainedModels/basic_numbies_model.pth\"\n",
    "\n",
    "## Important if you want to train again, set this to True\n",
    "trainAgain = False\n",
    "\n",
    "# Then we download training data, which my computer is probably gonna find highly\n",
    "# irresponsible\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root = \"../data\",\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = ToTensor(),\n",
    ")\n",
    "\n",
    "# And also downloading the test data\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root = \"../data\",\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = ToTensor(),\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Then we create something called data loaders:\n",
    "train_dataloader = DataLoader(training_data, batch_size = batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size = batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Models\n",
    "So when we create a neural network in pyTorch it is apparently just creating a class which inherits from\n",
    "the nn.Module class.\n",
    "If classes are a little unfamiliar check out this great [link](https://www.w3schools.com/python/python_classes.asp). If inheritence is also a little unfamiliar,\n",
    "this [link](https://www.w3schools.com/python/python_inheritance.asp) does wonders!\n",
    "So here is an example of defining a neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# I am uncertain how this exactly works, and it should probably be homework!\n",
    "# Get CPU or GPU for training\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the Model Parameters\n",
    "\n",
    "When we train a model we obviously need a loss function and an optimizer.\n",
    "\n",
    "Which is what we will be looking at here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.297588  [    0/60000]\n",
      "loss: 2.287777  [ 6400/60000]\n",
      "loss: 2.270909  [12800/60000]\n",
      "loss: 2.265343  [19200/60000]\n",
      "loss: 2.243504  [25600/60000]\n",
      "loss: 2.214704  [32000/60000]\n",
      "loss: 2.222090  [38400/60000]\n",
      "loss: 2.191419  [44800/60000]\n",
      "loss: 2.185217  [51200/60000]\n",
      "loss: 2.151105  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 43.0%, Avg loss: 2.146660 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.153535  [    0/60000]\n",
      "loss: 2.142316  [ 6400/60000]\n",
      "loss: 2.085518  [12800/60000]\n",
      "loss: 2.103630  [19200/60000]\n",
      "loss: 2.052236  [25600/60000]\n",
      "loss: 1.982849  [32000/60000]\n",
      "loss: 2.009181  [38400/60000]\n",
      "loss: 1.933348  [44800/60000]\n",
      "loss: 1.942038  [51200/60000]\n",
      "loss: 1.855419  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 56.6%, Avg loss: 1.861541 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.891853  [    0/60000]\n",
      "loss: 1.860249  [ 6400/60000]\n",
      "loss: 1.743510  [12800/60000]\n",
      "loss: 1.786175  [19200/60000]\n",
      "loss: 1.684457  [25600/60000]\n",
      "loss: 1.622479  [32000/60000]\n",
      "loss: 1.642162  [38400/60000]\n",
      "loss: 1.552702  [44800/60000]\n",
      "loss: 1.584251  [51200/60000]\n",
      "loss: 1.461398  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Avg loss: 1.488828 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.558385  [    0/60000]\n",
      "loss: 1.520722  [ 6400/60000]\n",
      "loss: 1.372070  [12800/60000]\n",
      "loss: 1.443668  [19200/60000]\n",
      "loss: 1.335769  [25600/60000]\n",
      "loss: 1.322826  [32000/60000]\n",
      "loss: 1.336413  [38400/60000]\n",
      "loss: 1.269413  [44800/60000]\n",
      "loss: 1.307672  [51200/60000]\n",
      "loss: 1.196747  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 1.227502 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.307965  [    0/60000]\n",
      "loss: 1.288002  [ 6400/60000]\n",
      "loss: 1.122688  [12800/60000]\n",
      "loss: 1.230666  [19200/60000]\n",
      "loss: 1.111904  [25600/60000]\n",
      "loss: 1.128066  [32000/60000]\n",
      "loss: 1.153360  [38400/60000]\n",
      "loss: 1.094052  [44800/60000]\n",
      "loss: 1.136670  [51200/60000]\n",
      "loss: 1.043934  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 1.068039 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# exists is a function from os.path (standard library)\n",
    "trained_model_exists = exists(saveModelPath)\n",
    "    \n",
    "# lr stands for learning rate\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "# In one instance of a training loop our network (the model) makes predictions\n",
    "# based on the trainning dataset (fed to the model in batches),\n",
    "# then it backpropagates the prediction error to adjust the model parameters (those we optimise)\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "# To ensure that the model is actually learning something, we test its performance\n",
    "# against the test dataset.\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "# We train if we haven't already trained\n",
    "# or we want to train again.\n",
    "if not trained_model_exists or trainAgain:\n",
    "    # Finally, the training process is conducted over several epochs (runs through the training set)\n",
    "    # We print the models loss and accuracy at each epoch, to show whether the procedure\n",
    "    # actually advances\n",
    "    epochs = 5\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "        test(test_dataloader, model, loss_fn)\n",
    "    print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving models\n",
    "\n",
    "Something about saving the model (very informative... I know)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to ../trainedModels/basic_numbies_model.pth\n"
     ]
    }
   ],
   "source": [
    "if not trained_model_exists or trainAgain:\n",
    "    torch.save(model.state_dict(), saveModelPath)\n",
    "    print(f\"Saved PyTorch Model State to {saveModelPath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading models\n",
    "\n",
    "Saving and loading models seems like two sides of the same coin... right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 'Ankle boot', Actual: 'Ankle boot'\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load(saveModelPath))\n",
    "\n",
    "# The loaded model can then be used to make predictions:\n",
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f\"Predicted: '{predicted}', Actual: '{actual}'\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c543ace77a335d6d53f78c966f120f62d739252c040c86805b5b43f8832d8b96"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('Bachelor_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

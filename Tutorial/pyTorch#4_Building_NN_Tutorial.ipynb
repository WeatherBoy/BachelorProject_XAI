{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILD THE NEURAL NETWORK\n",
    "\n",
    "This [tutorial](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html) is also given a webpage.\n",
    "\n",
    "From `torch.nn` we get all the building blocks required for building a neural network.\n",
    "Its probably best just to try, but basically a neural network in this is an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Device for Training\n",
    "\n",
    "We would very much like our Neural Networks to run on the GPU, because it is significantly faster. So that is what we wil start by trying to accomplish:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Class\n",
    "\n",
    "Then define the class (object) which is our neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # I believe that this \"Flatten\" function just flattens the\n",
    "        # data to a tensor which can be parsed through a network.\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Then this is actually the network.\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    # Here is the function that takes the data through the network.\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Here we just look at how it looks (yes.. very philosophical)    \n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use our neural network (model) we pass it the input data. Do not simply call `model.forward()` on the data, because some background (torch fancy) operations needs to be run.\n",
    "\n",
    "So we instead call `model()` on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([6])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "\n",
    "# I am unsure what \"logits\" is supossed to mean but it is basically the output\n",
    "# from our network.\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "\n",
    "# The softmax function here just rescales it to the domain [0,1].\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Layers\n",
    "\n",
    "What do the different layers of the model be doing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nn.Flatten()`\n",
    "As I *commented* at the code for the model. The flatten layer is simply put to flatten our data. In this case it makes great sense as the data is pictures. \n",
    "\n",
    "Here is given an example of the dimensionality of the data after flattening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the input image, before the 'flatten':\n",
      "torch.Size([3, 28, 28])\n",
      "\n",
      "The size of the image after 'flattening':\n",
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "# start out by creating a VERY random image\n",
    "input_image = torch.rand(3, 28, 28)\n",
    "print(\"The size of the input image, before the 'flatten':\")\n",
    "print(input_image.size())\n",
    "\n",
    "# just renaming the function\n",
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(\"\\nThe size of the image after 'flattening':\")\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ReLU and Linear layer\n",
    "\n",
    "I think that I understand what the Rectified Linear Unit and Linear functions does, roughly enough that I do not need to describe it here.\n",
    "\n",
    "Alas, moving on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Softmax()\n",
    "\n",
    "The model returns raw data in the domain [-infty, infty] (from negative infinity to positive infinity). This needs to be in the domain [0,1] to show the class which the model predicts each observation lies within. This is accomplished by the Softmax function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters\n",
    "\n",
    "This propably won't be very useful at any point, but might as well mark it down.\n",
    "\n",
    "After the model has trained how do we find the different weight which it has assigned?\n",
    "Well.. look no further than this snippet of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:  NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ") \n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0005, -0.0134, -0.0007,  ...,  0.0205, -0.0305, -0.0312],\n",
      "        [ 0.0282,  0.0194, -0.0033,  ...,  0.0139,  0.0097,  0.0239]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([0.0017, 0.0245], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0137,  0.0308,  0.0328,  ..., -0.0093,  0.0179, -0.0079],\n",
      "        [ 0.0141, -0.0300,  0.0300,  ...,  0.0326,  0.0406, -0.0295]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([ 0.0166, -0.0183], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0155, -0.0062, -0.0060,  ..., -0.0421,  0.0152, -0.0269],\n",
      "        [-0.0318,  0.0312, -0.0308,  ..., -0.0337,  0.0187, -0.0406]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0356,  0.0340], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Model structure: \", model, \"\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c543ace77a335d6d53f78c966f120f62d739252c040c86805b5b43f8832d8b96"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('Bachelor_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

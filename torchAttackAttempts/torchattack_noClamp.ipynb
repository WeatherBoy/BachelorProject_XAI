{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torchattack but without the clamping\n",
    "Simply the source code with minor modifications, source code found in [docs](https://adversarial-attacks-pytorch.readthedocs.io/en/latest/attacks.html) of the library. Only the attacks found interesting in initial analysis is modified:\n",
    "\n",
    "- BIM\n",
    "- FGSM\n",
    "- DeepFool\n",
    "- CW\n",
    "- PGDl2\n",
    "\n",
    "First import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from ..attack import Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FGSM_nc(Attack):\n",
    "    r\"\"\"\n",
    "    FGSM in the paper 'Explaining and harnessing adversarial examples'\n",
    "    [https://arxiv.org/abs/1412.6572]\n",
    "\n",
    "    Distance Measure : Linf\n",
    "\n",
    "    Arguments:\n",
    "        model (nn.Module): model to attack.\n",
    "        eps (float): maximum perturbation. (Default: 0.007)\n",
    "\n",
    "    Shape:\n",
    "        - images: :math:`(N, C, H, W)` where `N = number of batches`, `C = number of channels`,        `H = height` and `W = width`. It must have a range [0, 1].\n",
    "        - labels: :math:`(N)` where each value :math:`y_i` is :math:`0 \\leq y_i \\leq` `number of labels`.\n",
    "        - output: :math:`(N, C, H, W)`.\n",
    "\n",
    "    Examples::\n",
    "        >>> attack = torchattacks.FGSM(model, eps=0.007)\n",
    "        >>> adv_images = attack(images, labels)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, model, eps=0.007):\n",
    "        super().__init__(\"FGSM\", model)\n",
    "        self.eps = eps\n",
    "        self._supported_mode = ['default', 'targeted']\n",
    "\n",
    "    def forward(self, images, labels):\n",
    "        r\"\"\"\n",
    "        Overridden.\n",
    "        \"\"\"\n",
    "        images = images.clone().detach().to(self.device)\n",
    "        labels = labels.clone().detach().to(self.device)\n",
    "\n",
    "        if self._targeted:\n",
    "            target_labels = self._get_target_label(images, labels)\n",
    "\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        images.requires_grad = True\n",
    "        outputs = self.model(images)\n",
    "\n",
    "        # Calculate loss\n",
    "        if self._targeted:\n",
    "            cost = -loss(outputs, target_labels)\n",
    "        else:\n",
    "            cost = loss(outputs, labels)\n",
    "\n",
    "        # Update adversarial images\n",
    "        grad = torch.autograd.grad(cost, images,\n",
    "                                   retain_graph=False, create_graph=False)[0]\n",
    "\n",
    "        adv_images = images + self.eps*grad.sign()\n",
    "        noise = self.eps*grad.sign()\n",
    "        # adv_images = torch.clamp(adv_images, min=0, max=1).detach()\n",
    "\n",
    "        return adv_images, noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIM_nc(Attack):\n",
    "    r\"\"\"\n",
    "    BIM or iterative-FGSM in the paper 'Adversarial Examples in the Physical World'\n",
    "    [https://arxiv.org/abs/1607.02533]\n",
    "\n",
    "    Distance Measure : Linf\n",
    "\n",
    "    Arguments:\n",
    "        model (nn.Module): model to attack.\n",
    "        eps (float): maximum perturbation. (Default: 4/255)\n",
    "        alpha (float): step size. (Default: 1/255)\n",
    "        steps (int): number of steps. (Default: 0)\n",
    "\n",
    "    .. note:: If steps set to 0, steps will be automatically decided following the paper.\n",
    "\n",
    "    Shape:\n",
    "        - images: :math:`(N, C, H, W)` where `N = number of batches`, `C = number of channels`,        `H = height` and `W = width`. It must have a range [0, 1].\n",
    "        - labels: :math:`(N)` where each value :math:`y_i` is :math:`0 \\leq y_i \\leq` `number of labels`.\n",
    "        - output: :math:`(N, C, H, W)`.\n",
    "\n",
    "    Examples::\n",
    "        >>> attack = torchattacks.BIM(model, eps=4/255, alpha=1/255, steps=0)\n",
    "        >>> adv_images = attack(images, labels)\n",
    "    \"\"\"\n",
    "    def __init__(self, model, eps=4/255, alpha=1/255, steps=0):\n",
    "        super().__init__(\"BIM\", model)\n",
    "        self.eps = eps\n",
    "        self.alpha = alpha\n",
    "        if steps == 0:\n",
    "            self.steps = int(min(eps*255 + 4, 1.25*eps*255))\n",
    "        else:\n",
    "            self.steps = steps\n",
    "        self._supported_mode = ['default', 'targeted']\n",
    "\n",
    "    def forward(self, images, labels):\n",
    "        r\"\"\"\n",
    "        Overridden.\n",
    "        \"\"\"\n",
    "        images = images.clone().detach().to(self.device)\n",
    "        labels = labels.clone().detach().to(self.device)\n",
    "\n",
    "        if self._targeted:\n",
    "            target_labels = self._get_target_label(images, labels)\n",
    "\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        ori_images = images.clone().detach()\n",
    "\n",
    "        for _ in range(self.steps):\n",
    "            images.requires_grad = True\n",
    "            outputs = self.model(images)\n",
    "\n",
    "            # Calculate loss\n",
    "            if self._targeted:\n",
    "                cost = -loss(outputs, target_labels)\n",
    "            else:\n",
    "                cost = loss(outputs, labels)\n",
    "\n",
    "            # Update adversarial images\n",
    "            grad = torch.autograd.grad(cost, images,\n",
    "                                       retain_graph=False,\n",
    "                                       create_graph=False)[0]\n",
    "\n",
    "            adv_images = images + self.alpha*grad.sign()\n",
    "            a = ori_images - self.eps  #torch.clamp(ori_images - self.eps, min=0)\n",
    "            b = (adv_images >= a).float()*adv_images \\\n",
    "                + (adv_images < a).float()*a\n",
    "            c = (b > ori_images+self.eps).float()*(ori_images+self.eps) \\\n",
    "                + (b <= ori_images + self.eps).float()*b\n",
    "            images = c.detach() #torch.clamp(c, max=1).detach()\n",
    "        \n",
    "        noise = images - ori_images\n",
    "\n",
    "        return images, noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CW\n",
    "cannot find clamp...? check again later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CW_nc(Attack):\n",
    "    r\"\"\"\n",
    "    CW in the paper 'Towards Evaluating the Robustness of Neural Networks'\n",
    "    [https://arxiv.org/abs/1608.04644]\n",
    "\n",
    "    Distance Measure : L2\n",
    "\n",
    "    Arguments:\n",
    "        model (nn.Module): model to attack.\n",
    "        c (float): c in the paper. parameter for box-constraint. (Default: 1e-4)    \n",
    "            :math:`minimize \\Vert\\frac{1}{2}(tanh(w)+1)-x\\Vert^2_2+c\\cdot f(\\frac{1}{2}(tanh(w)+1))`\n",
    "        kappa (float): kappa (also written as 'confidence') in the paper. (Default: 0)\n",
    "            :math:`f(x')=max(max\\{Z(x')_i:i\\neq t\\} -Z(x')_t, - \\kappa)`\n",
    "        steps (int): number of steps. (Default: 1000)\n",
    "        lr (float): learning rate of the Adam optimizer. (Default: 0.01)\n",
    "\n",
    "    .. warning:: With default c, you can't easily get adversarial images. Set higher c like 1.\n",
    "\n",
    "    Shape:\n",
    "        - images: :math:`(N, C, H, W)` where `N = number of batches`, `C = number of channels`,        `H = height` and `W = width`. It must have a range [0, 1].\n",
    "        - labels: :math:`(N)` where each value :math:`y_i` is :math:`0 \\leq y_i \\leq` `number of labels`.\n",
    "        - output: :math:`(N, C, H, W)`.\n",
    "\n",
    "    Examples::\n",
    "        >>> attack = torchattacks.CW(model, c=1e-4, kappa=0, steps=1000, lr=0.01)\n",
    "        >>> adv_images = attack(images, labels)\n",
    "\n",
    "    .. note:: Binary search for c is NOT IMPLEMENTED methods in the paper due to time consuming.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, model, c=1e-4, kappa=0, steps=1000, lr=0.01):\n",
    "        super().__init__(\"CW\", model)\n",
    "        self.c = c\n",
    "        self.kappa = kappa\n",
    "        self.steps = steps\n",
    "        self.lr = lr\n",
    "        self._supported_mode = ['default', 'targeted']\n",
    "\n",
    "    def forward(self, images, labels):\n",
    "        r\"\"\"\n",
    "        Overridden.\n",
    "        \"\"\"\n",
    "        images = images.clone().detach().to(self.device)\n",
    "        labels = labels.clone().detach().to(self.device)\n",
    "\n",
    "        if self._targeted:\n",
    "            target_labels = self._get_target_label(images, labels)\n",
    "\n",
    "        # w = torch.zeros_like(images).detach() # Requires 2x times\n",
    "        w = self.inverse_tanh_space(images).detach()\n",
    "        w.requires_grad = True\n",
    "\n",
    "        best_adv_images = images.clone().detach()\n",
    "        best_L2 = 1e10*torch.ones((len(images))).to(self.device)\n",
    "        prev_cost = 1e10\n",
    "        dim = len(images.shape)\n",
    "\n",
    "        MSELoss = nn.MSELoss(reduction='none')\n",
    "        Flatten = nn.Flatten()\n",
    "\n",
    "        optimizer = optim.Adam([w], lr=self.lr)\n",
    "\n",
    "        for step in range(self.steps):\n",
    "            # Get adversarial images\n",
    "            adv_images = self.tanh_space(w)\n",
    "\n",
    "            # Calculate loss\n",
    "            current_L2 = MSELoss(Flatten(adv_images),\n",
    "                                 Flatten(images)).sum(dim=1)\n",
    "            L2_loss = current_L2.sum()\n",
    "\n",
    "            outputs = self.model(adv_images)\n",
    "            if self._targeted:\n",
    "                f_loss = self.f(outputs, target_labels).sum()\n",
    "            else:\n",
    "                f_loss = self.f(outputs, labels).sum()\n",
    "\n",
    "            cost = L2_loss + self.c*f_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            cost.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update adversarial images\n",
    "            _, pre = torch.max(outputs.detach(), 1)\n",
    "            correct = (pre == labels).float()\n",
    "\n",
    "            # filter out images that get either correct predictions or non-decreasing loss, \n",
    "            # i.e., only images that are both misclassified and loss-decreasing are left \n",
    "            mask = (1-correct)*(best_L2 > current_L2.detach())\n",
    "            best_L2 = mask*current_L2.detach() + (1-mask)*best_L2\n",
    "\n",
    "            mask = mask.view([-1]+[1]*(dim-1))\n",
    "            best_adv_images = mask*adv_images.detach() + (1-mask)*best_adv_images\n",
    "\n",
    "            # Early stop when loss does not converge.\n",
    "            # max(.,1) To prevent MODULO BY ZERO error in the next step.\n",
    "            if step % max(self.steps//10,1) == 0:\n",
    "                if cost.item() > prev_cost:\n",
    "                    return best_adv_images\n",
    "                prev_cost = cost.item()\n",
    "\n",
    "        return best_adv_images\n",
    "\n",
    "\n",
    "    def tanh_space(self, x):\n",
    "        return 1/2*(torch.tanh(x) + 1)\n",
    "\n",
    "    def inverse_tanh_space(self, x):\n",
    "        # torch.atanh is only for torch >= 1.7.0\n",
    "        return self.atanh(x*2-1)\n",
    "\n",
    "    def atanh(self, x):\n",
    "        return 0.5*torch.log((1+x)/(1-x))\n",
    "\n",
    "    # f-function in the paper\n",
    "    def f(self, outputs, labels):\n",
    "        one_hot_labels = torch.eye(len(outputs[0]))[labels].to(self.device)\n",
    "\n",
    "        i, _ = torch.max((1-one_hot_labels)*outputs, dim=1) # get the second largest logit\n",
    "        j = torch.masked_select(outputs, one_hot_labels.bool()) # get the largest logit\n",
    "\n",
    "        if self._targeted:\n",
    "            return torch.clamp((i-j), min=-self.kappa)\n",
    "        else:\n",
    "            return torch.clamp((j-i), min=-self.kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PGD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGDL2_nc(Attack):\n",
    "    r\"\"\"\n",
    "    PGD in the paper 'Towards Deep Learning Models Resistant to Adversarial Attacks'\n",
    "    [https://arxiv.org/abs/1706.06083]\n",
    "\n",
    "    Distance Measure : L2\n",
    "\n",
    "    Arguments:\n",
    "        model (nn.Module): model to attack.\n",
    "        eps (float): maximum perturbation. (Default: 1.0)\n",
    "        alpha (float): step size. (Default: 0.2)\n",
    "        steps (int): number of steps. (Default: 40)\n",
    "        random_start (bool): using random initialization of delta. (Default: True)\n",
    "\n",
    "    Shape:\n",
    "        - images: :math:`(N, C, H, W)` where `N = number of batches`, `C = number of channels`,        `H = height` and `W = width`. It must have a range [0, 1].\n",
    "        - labels: :math:`(N)` where each value :math:`y_i` is :math:`0 \\leq y_i \\leq` `number of labels`.\n",
    "        - output: :math:`(N, C, H, W)`.\n",
    "\n",
    "    Examples::\n",
    "        >>> attack = torchattacks.PGDL2(model, eps=1.0, alpha=0.2, steps=40, random_start=True)\n",
    "        >>> adv_images = attack(images, labels)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, model, eps=1.0, alpha=0.2, steps=40, random_start=True, eps_for_division=1e-10):\n",
    "        super().__init__(\"PGDL2\", model)\n",
    "        self.eps = eps\n",
    "        self.alpha = alpha\n",
    "        self.steps = steps\n",
    "        self.random_start = random_start\n",
    "        self.eps_for_division = eps_for_division\n",
    "        self._supported_mode = ['default', 'targeted']\n",
    "\n",
    "    def forward(self, images, labels):\n",
    "        r\"\"\"\n",
    "        Overridden.\n",
    "        \"\"\"\n",
    "        images = images.clone().detach().to(self.device)\n",
    "        labels = labels.clone().detach().to(self.device)\n",
    "\n",
    "        if self._targeted:\n",
    "            target_labels = self._get_target_label(images, labels)\n",
    "\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        adv_images = images.clone().detach()\n",
    "        batch_size = len(images)\n",
    "\n",
    "        if self.random_start:\n",
    "            # Starting at a uniformly random point\n",
    "            delta = torch.empty_like(adv_images).normal_()\n",
    "            d_flat = delta.view(adv_images.size(0),-1)\n",
    "            n = d_flat.norm(p=2,dim=1).view(adv_images.size(0),1,1,1)\n",
    "            r = torch.zeros_like(n).uniform_(0, 1)\n",
    "            delta *= r/n*self.eps\n",
    "\n",
    "        for _ in range(self.steps):\n",
    "            adv_images.requires_grad = True\n",
    "            outputs = self.model(adv_images)\n",
    "\n",
    "            # Calculate loss\n",
    "            if self._targeted:\n",
    "                cost = -loss(outputs, target_labels)\n",
    "            else:\n",
    "                cost = loss(outputs, labels)\n",
    "\n",
    "            # Update adversarial images\n",
    "            grad = torch.autograd.grad(cost, adv_images,\n",
    "                                       retain_graph=False, create_graph=False)[0]\n",
    "            grad_norms = torch.norm(grad.view(batch_size, -1), p=2, dim=1) + self.eps_for_division\n",
    "            grad = grad / grad_norms.view(batch_size, 1, 1, 1)\n",
    "            adv_images = adv_images.detach() + self.alpha * grad\n",
    "\n",
    "            delta = adv_images - images\n",
    "            delta_norms = torch.norm(delta.view(batch_size, -1), p=2, dim=1)\n",
    "            factor = self.eps / delta_norms\n",
    "            factor = torch.min(factor, torch.ones_like(delta_norms))\n",
    "            delta = delta * factor.view(-1, 1, 1, 1)\n",
    "\n",
    "            adv_images = images + delta#torch.clamp(images + delta, min=0, max=1).detach()\n",
    "        \n",
    "        noise = adv_images - images\n",
    "\n",
    "        return adv_images, noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deepfool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFool_nc(Attack):\n",
    "    r\"\"\"\n",
    "    'DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks'\n",
    "    [https://arxiv.org/abs/1511.04599]\n",
    "\n",
    "    Distance Measure : L2\n",
    "\n",
    "    Arguments:\n",
    "        model (nn.Module): model to attack.\n",
    "        steps (int): number of steps. (Default: 50)\n",
    "        overshoot (float): parameter for enhancing the noise. (Default: 0.02)\n",
    "\n",
    "    Shape:\n",
    "        - images: :math:`(N, C, H, W)` where `N = number of batches`, `C = number of channels`,        `H = height` and `W = width`. It must have a range [0, 1].\n",
    "        - labels: :math:`(N)` where each value :math:`y_i` is :math:`0 \\leq y_i \\leq` `number of labels`.\n",
    "        - output: :math:`(N, C, H, W)`.\n",
    "\n",
    "    Examples::\n",
    "        >>> attack = torchattacks.DeepFool(model, steps=50, overshoot=0.02)\n",
    "        >>> adv_images = attack(images, labels)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, model, steps=50, overshoot=0.02):\n",
    "        super().__init__(\"DeepFool\", model)\n",
    "        self.steps = steps\n",
    "        self.overshoot = overshoot\n",
    "        self._supported_mode = ['default']\n",
    "\n",
    "    def forward(self, images, labels, return_target_labels=False):\n",
    "        r\"\"\"\n",
    "        Overridden.\n",
    "        \"\"\"\n",
    "        images = images.clone().detach().to(self.device)\n",
    "        labels = labels.clone().detach().to(self.device)\n",
    "\n",
    "        batch_size = len(images)\n",
    "        correct = torch.tensor([True]*batch_size)\n",
    "        target_labels = labels.clone().detach().to(self.device)\n",
    "        curr_steps = 0\n",
    "\n",
    "        adv_images = []\n",
    "        for idx in range(batch_size):\n",
    "            image = images[idx:idx+1].clone().detach()\n",
    "            adv_images.append(image)\n",
    "\n",
    "        while (True in correct) and (curr_steps < self.steps):\n",
    "            for idx in range(batch_size):\n",
    "                if not correct[idx]: continue\n",
    "                early_stop, pre, adv_image = self._forward_indiv(adv_images[idx], labels[idx])\n",
    "                adv_images[idx] = adv_image\n",
    "                target_labels[idx] = pre\n",
    "                if early_stop:\n",
    "                    correct[idx] = False\n",
    "            curr_steps += 1\n",
    "\n",
    "        adv_images = torch.cat(adv_images).detach()\n",
    "\n",
    "        if return_target_labels:\n",
    "            return adv_images, target_labels\n",
    "\n",
    "        return adv_images, adv_image - images\n",
    "\n",
    "\n",
    "    def _forward_indiv(self, image, label):\n",
    "        image.requires_grad = True\n",
    "        fs = self.model(image)[0]\n",
    "        _, pre = torch.max(fs, dim=0)\n",
    "        if pre != label:\n",
    "            return (True, pre, image)\n",
    "\n",
    "        ws = self._construct_jacobian(fs, image)\n",
    "        image = image.detach()\n",
    "\n",
    "        f_0 = fs[label]\n",
    "        w_0 = ws[label]\n",
    "\n",
    "        wrong_classes = [i for i in range(len(fs)) if i != label]\n",
    "        f_k = fs[wrong_classes]\n",
    "        w_k = ws[wrong_classes]\n",
    "\n",
    "        f_prime = f_k - f_0\n",
    "        w_prime = w_k - w_0\n",
    "        value = torch.abs(f_prime) \\\n",
    "                / torch.norm(nn.Flatten()(w_prime), p=2, dim=1)\n",
    "        _, hat_L = torch.min(value, 0)\n",
    "\n",
    "        delta = (torch.abs(f_prime[hat_L])*w_prime[hat_L] \\\n",
    "                 / (torch.norm(w_prime[hat_L], p=2)**2))\n",
    "\n",
    "        target_label = hat_L if hat_L < label else hat_L+1\n",
    "\n",
    "        adv_image = image + (1+self.overshoot)*delta\n",
    "        #adv_image = torch.clamp(adv_image, min=0, max=1).detach()\n",
    "        return (False, target_label, adv_image)\n",
    "\n",
    "    # https://stackoverflow.com/questions/63096122/pytorch-is-it-possible-to-differentiate-a-matrix\n",
    "    # torch.autograd.functional.jacobian is only for torch >= 1.5.1\n",
    "    def _construct_jacobian(self, y, x):\n",
    "        x_grads = []\n",
    "        for idx, y_element in enumerate(y):\n",
    "            if x.grad is not None:\n",
    "                x.grad.zero_()\n",
    "            y_element.backward(retain_graph=(False or idx+1 < len(y)))\n",
    "            x_grads.append(x.grad.clone().detach())\n",
    "        return torch.stack(x_grads).reshape(*y.shape, *x.shape)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d806fedfadc4f43df997bb69e69664c5753a645548adea2a58a56079e3db770c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
